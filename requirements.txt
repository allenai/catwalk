ai2-tango[torch,transformers,fairscale,beaker,wandb]>=1.1
#ai2-tango[torch,transformers,fairscale,beaker,wandb] @ git+https://github.com/allenai/tango.git@main

torchmetrics==0.11.0
more_itertools
spacy>=3.0.0
wget
datasets>=2.1.0
accelerate
bettermap

# For the P3 datasets, which we get from huggingface datasets
protobuf<=3.20

# For lm-eval
sacrebleu>=1.5.0
scikit-learn>=0.24.1   # Eleuther uses this for metrics. Can we replace it with torchmetrics?
pycountry>=20.7.3
rouge-score>=0.0.4  # Can we replace this with torchmetrics?
# The Eleuther test harness depends on these even at runtime, but does not declare them.
mypy_extensions
pytest

# For promptsource
jinja2
pyyaml>=5
